{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad274da1-82b6-4974-bd52-22983ea73ad9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.nn.parallel import DataParallel\n",
    "from torchvision.transforms import ToTensor, Compose\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import os\n",
    "from torchmetrics.classification import BinaryJaccardIndex\n",
    "\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "\n",
    "from datasets import Embedding_Dataset, Custom_Dataset, Cutout_Split_Dataset\n",
    "from utils import SAMPreprocess, PILToNumpy, NumpyToTensor, SamplePoint, embedding_collate, is_valid_file\n",
    "from utils import create_cutout_split\n",
    "from models import SAM_Baseline\n",
    "\n",
    "jaccard = BinaryJaccardIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b0e2256-94f7-4f98-99bf-c4d6ceeb318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "folder_paths = [\n",
    "    #'/pfs/work7/workspace/scratch/ul_xto11-FSSAM/Liebherr/dataset',\n",
    "    '/pfs/work7/workspace/scratch/ul_xto11-FSSAM/FSSAM Datasets/EgoHOS/dataset',\n",
    "    '/pfs/work7/workspace/scratch/ul_xto11-FSSAM/FSSAM Datasets/GTEA/dataset',\n",
    "    '/pfs/work7/workspace/scratch/ul_xto11-FSSAM/FSSAM Datasets/LVIS/dataset',\n",
    "    '/pfs/work7/workspace/scratch/ul_xto11-FSSAM/FSSAM Datasets/NDIS Park/dataset',\n",
    "    '/pfs/work7/workspace/scratch/ul_xto11-FSSAM/FSSAM Datasets/TrashCan/dataset',\n",
    "    '/pfs/work7/workspace/scratch/ul_xto11-FSSAM/FSSAM Datasets/ZeroWaste-f/dataset',\n",
    "]\n",
    "select_mode = 'max_sim' # must be one of: 'random', 'first', 'highest_pred', 'max_sim'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed91e000-6e9e-477b-b577-192a81646822",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SAM_Baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb7ac1e3-71e6-488d-9129-410103c2c064",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP loaded 224\n"
     ]
    }
   ],
   "source": [
    "from maskCLIP.clip import clip # modified version of CLIP from ov-seg, adds mask-embedding to image\n",
    "clip_model, clip_preprocess = clip.load('ViT-L/14', mask_prompt_depth=3, device=device)\n",
    "ov_seg = torch.load('ovseg_swinbase_vitL14_ft_mpt.pth')\n",
    "state_dict = ov_seg[\"model\"]\n",
    "state_dict = {\n",
    "    k.replace(\"clip_adapter.clip_model.\", \"\"): v\n",
    "    for k, v in state_dict.items()\n",
    "    if k.startswith(\"clip_adapter.clip_model.\")\n",
    "}\n",
    "clip_model.load_state_dict(state_dict)\n",
    "cutout_size = clip_model.visual.input_resolution\n",
    "print(\"CLIP loaded\", cutout_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8190aec1-2a6c-4dfd-b69f-158a3c4404eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PIXEL_MEAN = (0.48145466, 0.4578275, 0.40821073)\n",
    "PIXEL_STD = (0.26862954, 0.26130258, 0.27577711)\n",
    "pixel_mean = torch.Tensor(PIXEL_MEAN).reshape(3, 1, 1)\n",
    "pixel_std = torch.Tensor(PIXEL_STD).reshape(3, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "250fd898-9523-4679-8dee-6e9c34649563",
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_transform = ResizeLongestSide(model.img_size)\n",
    "target_transform = Compose([\n",
    "    sam_transform.apply_image_torch, # rescale\n",
    "    SAMPreprocess(model.img_size, normalize=False), # padding\n",
    "    SamplePoint(),\n",
    "])\n",
    "transform = Compose([\n",
    "    PILToNumpy(),\n",
    "    sam_transform.apply_image, # rescale\n",
    "    NumpyToTensor(),\n",
    "    SAMPreprocess(model.img_size) # padding\n",
    "])\n",
    "example_transform = Compose([\n",
    "    ToTensor(),\n",
    "])\n",
    "\n",
    "def create_dataloader(folder_path):\n",
    "    dataset = Embedding_Dataset(root=folder_path, transform=transform, target_transform=target_transform, is_valid_file=is_valid_file)\n",
    "    example_set = Custom_Dataset(root=folder_path, transform=example_transform, is_valid_file=is_valid_file)\n",
    "\n",
    "    dataset_size = len(dataset)\n",
    "    example_size = int(0.25 * dataset_size)\n",
    "    test_size = dataset_size - example_size\n",
    "    generator = torch.Generator().manual_seed(42)\n",
    "    test_indices, example_indices = random_split(range(len(dataset)), [test_size, example_size], generator=generator)\n",
    "\n",
    "    test_set = Subset(dataset, test_indices)\n",
    "    example_set = Subset(example_set, example_indices)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, collate_fn=embedding_collate)\n",
    "    return test_loader, example_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b2eba0c-60c7-4a04-aaa9-13eaef982a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EgoHOS loaded\n",
      "GTEA loaded\n",
      "LVIS loaded\n",
      "NDIS Park loaded\n",
      "TrashCan loaded\n",
      "ZeroWaste-f loaded\n"
     ]
    }
   ],
   "source": [
    "dataloaders = {}\n",
    "example_sets = {}\n",
    "\n",
    "for folder_path in folder_paths:\n",
    "    dataset_name = folder_path.split('/')[-2]  # Extract dataset name from the folder path\n",
    "    dataloaders[dataset_name], example_sets[dataset_name] = create_dataloader(folder_path)\n",
    "    print(dataset_name, \"loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b416e2d5-ab38-4e5e-9cd0-00929db90a97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import GaussianBlur\n",
    "\n",
    "class CreateCutouts(object):\n",
    "    def __init__(self, cutout_size, padding):\n",
    "        self.cutout_size = cutout_size\n",
    "        self.padding = padding\n",
    "\n",
    "    def __call__(self, image, masks):\n",
    "        #image = (image - pixel_mean) / pixel_std # better performance without normilization even in this setting\n",
    "        return create_cutout_split(image, masks, self.cutout_size, self.padding)\n",
    "\n",
    "create_cutouts_f = CreateCutouts(cutout_size, 30)\n",
    "\n",
    "#image, masks = example_set[1]\n",
    "#image_s, mask_s = create_cutouts_f(image, masks)\n",
    "#fig, axs = plt.subplots(1, 2)\n",
    "#axs[0].imshow(image_s[0].permute(1, 2, 0))\n",
    "#axs[1].imshow(mask_s[0].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86489a7b-a2da-44c5-ac69-79ff00718c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  torch.cuda.amp import autocast\n",
    "\n",
    "def get_cutouts_dataset(dataset):\n",
    "    images_set = []\n",
    "    masks_set = []\n",
    "    for image, masks in tqdm.tqdm(dataset):\n",
    "        i, m = create_cutouts_f(image, masks)\n",
    "        images_set.append(i)\n",
    "        masks_set.append(m)\n",
    "    cutouts_set = Cutout_Split_Dataset(torch.cat(images_set), torch.cat(masks_set))\n",
    "    return cutouts_set\n",
    "\n",
    "def get_cutout_embeddings(dataset, model, device, batch_size):\n",
    "    cutouts_loader = DataLoader(dataset, batch_size=batch_size)\n",
    "    examples = []\n",
    "    for images, masks in tqdm.tqdm(cutouts_loader):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        with autocast():\n",
    "            cutout_embeddings = model.visual(images, masks)\n",
    "        cutout_embeddings = cutout_embeddings[~torch.any(cutout_embeddings.isnan(), dim=1)] # remove nan embeddings\n",
    "        examples.append(cutout_embeddings.detach().cpu())\n",
    "    return torch.cat(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d35ac2b-220e-4b26-8d96-e514109fcd88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_example_embeddings(example_set):\n",
    "    file = 'example_embeddings/' + name + '_mpt.pt'\n",
    "\n",
    "    if os.path.exists(file):\n",
    "        example_embeddings = torch.load(file)\n",
    "    else:\n",
    "        cutouts_set = get_cutouts_dataset(example_set)\n",
    "        example_embeddings = get_cutout_embeddings(cutouts_set, clip_model, device, batch_size=16)\n",
    "        torch.save(example_embeddings, file)\n",
    "        \n",
    "    print(example_embeddings.shape)\n",
    "    return example_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "516a271d-fe82-4eeb-a640-5f68e911b4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest(images, masks, examples, create_cutouts_f, model, device):\n",
    "    embeddings = []\n",
    "    for image, mask_suggestions in zip(images, masks):\n",
    "        i, m = create_cutouts_f(image, mask_suggestions)\n",
    "        i, m = i.to(device), m.to(device)\n",
    "        with autocast():\n",
    "            cutout_embeddings = model.visual(i, m)\n",
    "        embeddings.append(cutout_embeddings.detach())\n",
    "    embeddings = torch.cat(embeddings)\n",
    "    examples = examples.to(device)\n",
    "    \n",
    "    embeddings /= torch.norm(embeddings, dim=1, keepdim=True)\n",
    "    examples /= torch.norm(examples, dim=1, keepdim=True)\n",
    "    similarity_matrix = embeddings @ examples.T\n",
    "    \n",
    "    similarity_matrix = similarity_matrix.reshape(len(images), 3, len(examples))\n",
    "    similarity_matrix, _ = torch.max(similarity_matrix, dim=-1)\n",
    "    max_i = torch.argmax(similarity_matrix, dim=-1)\n",
    "    return max_i.cpu().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2715068-f052-4b8a-ae21-c91f86f9bb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAM loaded\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = DataParallel(model)\n",
    "model.to(device)\n",
    "print(\"SAM loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d58a8a23-2806-450d-8633-0ac87c3bbbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, example_embeddings, device): # select_mode, create_cutouts_f, clip_model\n",
    "    np.random.seed(42)\n",
    "    model.eval()\n",
    "    total_iou = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm.tqdm(dataloader)\n",
    "        for images, masks, points, embeddings in pbar:\n",
    "            embeddings = embeddings.to(device)\n",
    "            points = points.to(device)\n",
    "\n",
    "            pred_masks, pred_iou = model(embeddings, points)\n",
    "            pred_masks, pred_iou = pred_masks.cpu(), pred_iou.cpu()\n",
    "            pred_masks = pred_masks > 0\n",
    "            \n",
    "            if select_mode == 'random':\n",
    "                max_i = torch.randint(3, (len(images),))\n",
    "            elif select_mode == 'first':\n",
    "                max_i = torch.zeros((len(images),), dtype=int)\n",
    "            elif select_mode == 'highest_pred':\n",
    "                max_i = torch.argmax(pred_iou, dim=1)\n",
    "            elif select_mode == 'max_sim':\n",
    "                max_i = get_closest(images, pred_masks, example_embeddings, create_cutouts_f, clip_model, device)\n",
    "\n",
    "            pred_masks = pred_masks[range(len(pred_masks)), max_i].unsqueeze(1)\n",
    "            iou = jaccard(masks, pred_masks)\n",
    "\n",
    "            total_iou += iou * images.size(0)\n",
    "            total_samples += images.size(0)\n",
    "            \n",
    "            pbar.set_postfix({'IoU': (total_iou / total_samples).item()})\n",
    "\n",
    "    average_iou = total_iou / total_samples\n",
    "    return average_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa1d8597-68ff-4630-9f41-67c0ab4eca43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading example embeddings for EgoHOS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:24<00:00, 10.27it/s]\n",
      "100%|██████████| 53/53 [00:04<00:00, 10.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([842, 768])\n",
      "Testing EgoHOS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [03:08<00:00,  2.01s/it, IoU=0.511]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IoU: 0.5113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading example embeddings for GTEA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [00:02<00:00, 74.16it/s]\n",
      "100%|██████████| 19/19 [00:01<00:00, 15.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 768])\n",
      "Testing GTEA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [01:34<00:00,  1.52s/it, IoU=0.786]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IoU: 0.7859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading example embeddings for LVIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:10<00:00, 24.15it/s]\n",
      "100%|██████████| 194/194 [00:12<00:00, 15.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3088, 768])\n",
      "Testing LVIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [02:33<00:00,  1.64s/it, IoU=0.446]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IoU: 0.4462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading example embeddings for NDIS Park\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:13<00:00,  2.01it/s]\n",
      "100%|██████████| 59/59 [00:03<00:00, 15.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([915, 768])\n",
      "Testing NDIS Park\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:55<00:00,  5.05s/it, IoU=0.283]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IoU: 0.2826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading example embeddings for TrashCan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:02<00:00, 92.15it/s] \n",
      "100%|██████████| 44/44 [00:02<00:00, 15.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([699, 768])\n",
      "Testing TrashCan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [02:10<00:00,  1.38s/it, IoU=0.28] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IoU: 0.2800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading example embeddings for ZeroWaste-f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:35<00:00,  6.96it/s]\n",
      "100%|██████████| 35/35 [00:02<00:00, 16.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([548, 768])\n",
      "Testing ZeroWaste-f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [03:04<00:00,  1.96s/it, IoU=0.237]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IoU: 0.2371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for name, dataloader in dataloaders.items():\n",
    "    print(\"Loading example embeddings for\", name)\n",
    "    example_embeddings = get_example_embeddings(example_sets[name])\n",
    "    print(\"Testing\", name)\n",
    "    iou_score = evaluate_model(model, dataloader, example_embeddings, device)\n",
    "    print('IoU: {:.4f}'.format(iou_score), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6ab4fc3-b15b-4ef8-8db0-ece8f2f1bf39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42385\n"
     ]
    }
   ],
   "source": [
    "MPT = [0.5113, 0.7859, 0.4462, 0.2826, 0.2800, 0.2371]\n",
    "print(np.mean(MPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecd8dc6-efb6-4d27-8783-29b416e6b172",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupytervenv)",
   "language": "python",
   "name": "jupytervenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
