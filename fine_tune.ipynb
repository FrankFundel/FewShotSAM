{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa8f844a-2f60-43b8-83a4-867801f01dfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fine-tune SAM on Liebherr Dataset\n",
    "# https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/SAM/Fine_tune_SAM_(segment_anything)_on_a_custom_dataset.ipynb#scrollTo=XC35CzLxfdQU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79e28afd-6f8d-4a0c-8ff2-d8cf236775a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-27 21:40:59.505715: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-27 21:40:59.550824: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-27 21:41:00.686710: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import monai\n",
    "import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.nn.parallel import DataParallel\n",
    "from torchvision.transforms import ToTensor, Compose\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.nn.functional import threshold, normalize\n",
    "from torchmetrics.classification import BinaryJaccardIndex\n",
    "\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "\n",
    "from datasets import Embedding_Dataset\n",
    "from utils import SAMPreprocess, PILToNumpy, NumpyToTensor, sample_point, embedding_collate, is_valid_file\n",
    "from models import SAM_Fine_Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6302f5ef-2204-4d42-aebd-9596b499ab0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SAM_Fine_Tune()\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "# make sure we only compute gradients for mask decoder\n",
    "for name, param in model.sam_model.named_parameters():\n",
    "    if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
    "        param.requires_grad_(False)\n",
    "\n",
    "jaccard = BinaryJaccardIndex().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43cf0154-d392-4d8c-97bf-bf25e6e5a8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_transform = ResizeLongestSide(model.img_size)\n",
    "target_transform = Compose([\n",
    "    sam_transform.apply_image_torch, # rescale\n",
    "    SAMPreprocess(model.img_size, normalize=False), # padding\n",
    "    sample_point,\n",
    "])\n",
    "transform = Compose([\n",
    "    PILToNumpy(),\n",
    "    sam_transform.apply_image, # rescale\n",
    "    NumpyToTensor(),\n",
    "    SAMPreprocess(model.img_size) # padding\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5f88aa9-ac90-41f6-8160-496adbc54b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 8\n",
    "lr = 1e-5\n",
    "\n",
    "folder_path = '/pfs/work7/workspace/scratch/ul_xto11-FSSAM/Liebherr/dataset'\n",
    "dataset = Embedding_Dataset(root=folder_path, transform=transform, target_transform=target_transform, is_valid_file=is_valid_file)\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(0.7 * dataset_size)\n",
    "val_size = int(0.15 * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size], generator)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, collate_fn=embedding_collate)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, collate_fn=embedding_collate)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, collate_fn=embedding_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baba0429-fe3b-42f0-aa49-46cdc23b0518",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.sam_model.mask_decoder.parameters(), lr=lr, weight_decay=0)\n",
    "criterion = monai.losses.DiceFocalLoss(sigmoid=True, squared_pred=True, lambda_focal=20.) # maybe include_background=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "200595b3-3f11-40a6-87bf-d0dccc304d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, epoch, criterion, optimizer, dataloader, device):\n",
    "    model.train()\n",
    "    \n",
    "    num_batches = len(dataloader)\n",
    "    num_samples = len(dataloader.dataset)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_iou = 0.0\n",
    "\n",
    "    for batch, (images, masks, points, embeddings) in enumerate(tqdm.tqdm(dataloader)):\n",
    "        # Transfer Data to GPU if available\n",
    "        embeddings, points, masks = embeddings.to(device), points.to(device), masks.to(device)\n",
    "\n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward Pass\n",
    "        outputs = model(embeddings, points)\n",
    "\n",
    "        # Compute Loss\n",
    "        loss = criterion(outputs, masks)\n",
    "\n",
    "        # Calculate gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update Weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate Loss\n",
    "        running_loss += loss.item() * embeddings.size(0)\n",
    "        running_iou += jaccard(masks > 0, outputs > 0) * embeddings.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / num_samples\n",
    "    epoch_iou = running_iou / num_samples\n",
    "\n",
    "    return epoch_loss, epoch_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cec44f86-7dab-43f9-ae0a-0a937c2d2ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(model, epoch, criterion, optimizer, dataloader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    num_batches = len(dataloader)\n",
    "    num_samples = len(dataloader.dataset)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        running_loss = 0.0\n",
    "        running_iou = 0.0\n",
    "\n",
    "        for batch, (images, masks, points, embeddings) in enumerate(tqdm.tqdm(dataloader)):\n",
    "            # Transfer Data to GPU if available\n",
    "            embeddings, points, masks = embeddings.to(device), points.to(device), masks.to(device)\n",
    "\n",
    "            # Clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward Pass\n",
    "            outputs = model(embeddings, points)\n",
    "\n",
    "            # Compute Loss\n",
    "            loss = criterion(outputs, masks)\n",
    "\n",
    "            # Calculate Loss\n",
    "            running_loss += loss.item() * embeddings.size(0)\n",
    "            running_iou += jaccard(masks > 0, outputs > 0) * embeddings.size(0)\n",
    "            \n",
    "        epoch_loss = running_loss / num_samples\n",
    "        epoch_iou = running_iou / num_samples\n",
    "\n",
    "    return epoch_loss, epoch_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6e101c4-e131-4cf8-9fc2-2788f22ef06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfrankfundel\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/pfs/data5/home/ul/ul_student/ul_xto11/FewShotSAM/wandb/run-20230827_214120-6c1430gl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frankfundel/Fine-Tune-SAM/runs/6c1430gl' target=\"_blank\">unique-spaceship-17</a></strong> to <a href='https://wandb.ai/frankfundel/Fine-Tune-SAM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frankfundel/Fine-Tune-SAM' target=\"_blank\">https://wandb.ai/frankfundel/Fine-Tune-SAM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frankfundel/Fine-Tune-SAM/runs/6c1430gl' target=\"_blank\">https://wandb.ai/frankfundel/Fine-Tune-SAM/runs/6c1430gl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/frankfundel/Fine-Tune-SAM/runs/6c1430gl?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1552604e3dc0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb_config = {\n",
    "    \"epochs\": epochs,\n",
    "    \"lr\": lr,\n",
    "    \"batch_size\": batch_size,\n",
    "}\n",
    "\n",
    "wandb.init(project=\"Fine-Tune-SAM\", entity=\"frankfundel\", config=wandb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1701637-7529-4a2a-8bca-f73f79c3c396",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val_loss = np.inf\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    end = time.time()\n",
    "    print(f\"==================== Starting at epoch {epoch} ====================\", flush=True)\n",
    "    \n",
    "    train_loss, train_iou = train_epoch(model, epoch, criterion, optimizer, train_loader, device)\n",
    "    print('Training loss: {:.4f} IoU: {:.4f}'.format(train_loss, train_iou), flush=True)\n",
    "    \n",
    "    val_loss, val_iou = test_epoch(model, epoch, criterion, optimizer, val_loader, device)\n",
    "    print('Validation loss: {:.4f} IoU: {:.4f}'.format(val_loss, val_iou), flush=True)\n",
    "    \n",
    "    wandb.log({\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_iou\": train_iou,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_iou\": val_iou,\n",
    "    })\n",
    "    \n",
    "    if min_val_loss > val_loss:\n",
    "        print('val_loss decreased, saving model', flush=True)\n",
    "        min_val_loss = val_loss\n",
    "        \n",
    "        # Saving State Dict\n",
    "        torch.save(model.state_dict(), 'Fine-Tune-SAM.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd400c51-89ba-4fb4-9c27-4e95208922a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load after training\n",
    "model.load_state_dict(torch.load('Fine-Tune-SAM.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b464f87-0616-405d-9e66-53cb4359b3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:33<00:00,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7117 IoU: 0.3974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_iou = test_epoch(model, 0, criterion, optimizer, test_loader, device)\n",
    "print('Test loss: {:.4f} IoU: {:.4f}'.format(test_loss, test_iou), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "460ba378-f108-4164-9eed-d56bc188f30a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_iou</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_iou</td><td>0.39742</td></tr><tr><td>test_loss</td><td>0.71172</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">unique-spaceship-17</strong> at: <a href='https://wandb.ai/frankfundel/Fine-Tune-SAM/runs/6c1430gl' target=\"_blank\">https://wandb.ai/frankfundel/Fine-Tune-SAM/runs/6c1430gl</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230827_214120-6c1430gl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.log({\n",
    "    \"test_loss\": test_loss,\n",
    "    \"test_iou\": test_iou\n",
    "})\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637c78e4-eb91-4d1d-ba0f-71e0c5c3d3d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupytervenv)",
   "language": "python",
   "name": "jupytervenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
