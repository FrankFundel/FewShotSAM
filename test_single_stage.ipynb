{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad274da1-82b6-4974-bd52-22983ea73ad9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.nn.parallel import DataParallel\n",
    "from torchvision.transforms import ToTensor, Compose\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import os\n",
    "from torchmetrics.classification import BinaryJaccardIndex\n",
    "\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "\n",
    "from datasets import Embedding_Dataset, Custom_Dataset, Cutout_Dataset\n",
    "from utils import SAMPreprocess, PILToNumpy, NumpyToTensor, SamplePoint, embedding_collate, is_valid_file\n",
    "from utils import create_cutouts\n",
    "from models import SAM_Concat\n",
    "\n",
    "jaccard = BinaryJaccardIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b0e2256-94f7-4f98-99bf-c4d6ceeb318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "folder_paths = [\n",
    "    '/pfs/work7/workspace/scratch/ul_xto11-FSSAM/Liebherr/dataset',\n",
    "    '/pfs/work7/workspace/scratch/ul_xto11-FSSAM/FSSAM Datasets/EgoHOS/dataset',\n",
    "    '/pfs/work7/workspace/scratch/ul_xto11-FSSAM/FSSAM Datasets/GTEA/dataset',\n",
    "    '/pfs/work7/workspace/scratch/ul_xto11-FSSAM/FSSAM Datasets/LVIS/dataset',\n",
    "    '/pfs/work7/workspace/scratch/ul_xto11-FSSAM/FSSAM Datasets/NDIS Park/dataset',\n",
    "    '/pfs/work7/workspace/scratch/ul_xto11-FSSAM/FSSAM Datasets/TrashCan/dataset',\n",
    "    '/pfs/work7/workspace/scratch/ul_xto11-FSSAM/FSSAM Datasets/ZeroWaste-f/dataset',\n",
    "]\n",
    "embed_model_type = 'dino' # clip, dino\n",
    "guidance_method = 'concat' # concat, attn\n",
    "example_method = 'ground_truth' # ground_truth, noisy_ground_truth, retrieval\n",
    "multi_output = False\n",
    "visual_prompt_engineering = False\n",
    "\n",
    "load_weights = 'SAM_single_concat_dino_ground_truth.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed91e000-6e9e-477b-b577-192a81646822",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb7ac1e3-71e6-488d-9129-410103c2c064",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP loaded 336\n"
     ]
    }
   ],
   "source": [
    "if embed_model_type == 'clip':\n",
    "    import clip\n",
    "    class CLIP:\n",
    "        def __init__(self, device):\n",
    "            self.model, _ = clip.load(\"ViT-L/14@336px\", device=device)\n",
    "            self.model.eval()\n",
    "            self.model.to(device)\n",
    "        def __call__(self, image):\n",
    "            with torch.no_grad():\n",
    "                return self.model.encode_image(image)\n",
    "    embed_model = CLIP(device)\n",
    "    cutout_size = 336\n",
    "    example_dim = 768\n",
    "    print(\"CLIP loaded\", cutout_size)\n",
    "elif embed_model_type == 'dino':\n",
    "    class DINO:\n",
    "        def __init__(self, device):\n",
    "            self.model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14')\n",
    "            self.model.eval()\n",
    "            self.model.to(device)\n",
    "        def __call__(self, image):\n",
    "            with torch.no_grad():\n",
    "                return self.model(image)\n",
    "    embed_model = DINO(device)\n",
    "    cutout_size = 336\n",
    "    example_dim = 1024\n",
    "    print(\"DINO loaded\", cutout_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "793dde79-9d3e-43ea-9045-df2b9d6ad6dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if guidance_method == 'concat':\n",
    "    model = SAM_Concat(example_dim, multi_output)\n",
    "else:\n",
    "    pass\n",
    "pretrained_dict = torch.load(load_weights)\n",
    "pretrained_dict = {key.replace(\"module.\", \"\"): value for key, value in pretrained_dict.items()}\n",
    "model.load_state_dict(pretrained_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "250fd898-9523-4679-8dee-6e9c34649563",
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_transform = ResizeLongestSide(model.img_size)\n",
    "target_transform = Compose([\n",
    "    sam_transform.apply_image_torch, # rescale\n",
    "    SAMPreprocess(model.img_size, normalize=False), # padding\n",
    "    SamplePoint(),\n",
    "])\n",
    "transform = Compose([\n",
    "    PILToNumpy(),\n",
    "    sam_transform.apply_image, # rescale\n",
    "    NumpyToTensor(),\n",
    "    SAMPreprocess(model.img_size) # padding\n",
    "])\n",
    "example_transform = Compose([\n",
    "    ToTensor(),\n",
    "])\n",
    "\n",
    "def create_dataloader(folder_path):\n",
    "    dataset = Embedding_Dataset(root=folder_path, transform=transform, target_transform=target_transform, is_valid_file=is_valid_file)\n",
    "    example_set = Custom_Dataset(root=folder_path, transform=example_transform, is_valid_file=is_valid_file)\n",
    "\n",
    "    dataset_size = len(dataset)\n",
    "    example_size = int(0.25 * dataset_size)\n",
    "    test_size = dataset_size - example_size\n",
    "    generator = torch.Generator().manual_seed(42)\n",
    "    test_indices, example_indices = random_split(range(len(dataset)), [test_size, example_size], generator=generator)\n",
    "\n",
    "    test_set = Subset(dataset, test_indices)\n",
    "    example_set = Subset(example_set, example_indices)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, collate_fn=embedding_collate)\n",
    "    return test_loader, example_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1151c254-5b65-4551-acb8-12baac02e67c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liebherr loaded\n",
      "EgoHOS loaded\n",
      "GTEA loaded\n",
      "LVIS loaded\n",
      "NDIS Park loaded\n",
      "TrashCan loaded\n",
      "ZeroWaste-f loaded\n"
     ]
    }
   ],
   "source": [
    "dataloaders = {}\n",
    "example_sets = {}\n",
    "\n",
    "for folder_path in folder_paths:\n",
    "    dataset_name = folder_path.split('/')[-2]  # Extract dataset name from the folder path\n",
    "    dataloaders[dataset_name], example_sets[dataset_name] = create_dataloader(folder_path)\n",
    "    print(dataset_name, \"loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b416e2d5-ab38-4e5e-9cd0-00929db90a97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import GaussianBlur\n",
    "\n",
    "class CreateCutouts(object):\n",
    "    def __init__(self, cutout_size, padding, background_transform, background_intensity):\n",
    "        self.cutout_size = cutout_size\n",
    "        self.padding = padding\n",
    "        self.background_transform = background_transform\n",
    "        self.background_intensity = background_intensity\n",
    "\n",
    "    def __call__(self, image, masks):\n",
    "        #image = (image - pixel_mean) / pixel_std # better performance without normilization\n",
    "        return create_cutouts(image, masks, self.cutout_size, self.padding, self.background_transform, self.background_intensity)\n",
    "        \n",
    "if visual_prompt_engineering:\n",
    "    background_intensity = .1\n",
    "    background_transform = Compose([\n",
    "        GaussianBlur(11, 10)\n",
    "    ])\n",
    "else:\n",
    "    background_intensity = 0\n",
    "    background_transform = None\n",
    "\n",
    "create_cutouts_f = CreateCutouts(cutout_size, 30, background_transform, background_intensity)\n",
    "\n",
    "#image, masks = example_set[1]\n",
    "#cutouts = create_cutouts_f(image, masks)\n",
    "#plt.imshow(cutouts[0].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86489a7b-a2da-44c5-ac69-79ff00718c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cutouts_dataset(dataset):\n",
    "    cutouts_set = []\n",
    "    for image, masks in tqdm.tqdm(dataset):\n",
    "        cutouts = create_cutouts_f(image, masks)\n",
    "        cutouts_set.append(cutouts)\n",
    "    cutouts_set = Cutout_Dataset(torch.cat(cutouts_set))\n",
    "    return cutouts_set\n",
    "\n",
    "def get_cutout_embeddings(dataset, model, device, batch_size):\n",
    "    cutouts_loader = DataLoader(dataset, batch_size=batch_size)\n",
    "    examples = []\n",
    "    for cutouts in tqdm.tqdm(cutouts_loader):\n",
    "        cutouts = cutouts.to(device)\n",
    "        cutout_embeddings = model.encode_image(cutouts)\n",
    "        cutout_embeddings = cutout_embeddings[~torch.any(cutout_embeddings.isnan(), dim=1)] # remove nan embeddings\n",
    "        examples.append(cutout_embeddings.detach().cpu())\n",
    "    return torch.cat(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d35ac2b-220e-4b26-8d96-e514109fcd88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_example_embeddings(example_set):\n",
    "    if visual_prompt_engineering:\n",
    "        file = 'example_embeddings/' + name + '_vpe_' + embed_model_type + '.pt'\n",
    "    else:\n",
    "        file = 'example_embeddings/' + name + '_' + embed_model_type + '.pt'\n",
    "\n",
    "    if os.path.exists(file):\n",
    "        example_embeddings = torch.load(file)\n",
    "    else:\n",
    "        cutouts_set = get_cutouts_dataset(example_set)\n",
    "        example_embeddings = get_cutout_embeddings(cutouts_set, embed_model, device, batch_size=16)\n",
    "        torch.save(example_embeddings, file)\n",
    "        \n",
    "    print(example_embeddings.shape)\n",
    "    return example_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2715068-f052-4b8a-ae21-c91f86f9bb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAM loaded\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = DataParallel(model)\n",
    "model.to(device)\n",
    "print(\"SAM loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d58a8a23-2806-450d-8633-0ac87c3bbbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, example_embeddings, device): # select_mode, create_cutouts_f, embed_model\n",
    "    np.random.seed(42)\n",
    "    model.eval()\n",
    "    total_iou = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm.tqdm(dataloader)\n",
    "        for images, masks, points, embeddings in pbar:\n",
    "            embeddings = embeddings.to(device)\n",
    "            points = points.to(device)\n",
    "            \n",
    "            # Get cutout embeddings\n",
    "            cutouts = [create_cutouts_f(i, m) for i, m in zip(images, masks)]\n",
    "            cutouts = torch.cat(cutouts).to(device)\n",
    "            cutout_embeddings = embed_model(cutouts)\n",
    "            \n",
    "            examples = cutout_embeddings.unsqueeze(1)\n",
    "            #examples = example_embeddings.repeat(len(images), 1, 1)[:, 0, :].unsqueeze(1)\n",
    "            #examples = torch.zeros((len(images), 1, 768), dtype=torch.float16).to(device)\n",
    "            \n",
    "            # Forward Pass\n",
    "            if multi_output:\n",
    "                outputs, iou_pred = model(embeddings, points, examples)\n",
    "                outputs = outputs[range(len(outputs)), torch.argmax(iou_pred, dim=1)].unsqueeze(1)\n",
    "            else:\n",
    "                outputs = model(embeddings, points, examples)\n",
    "\n",
    "            total_iou += jaccard(masks > 0, outputs.cpu() > 0) * embeddings.size(0)\n",
    "            total_samples += images.size(0)\n",
    "            \n",
    "            pbar.set_postfix({'IoU': (total_iou / total_samples).item()})\n",
    "\n",
    "    average_iou = total_iou / total_samples\n",
    "    return average_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1d8597-68ff-4630-9f41-67c0ab4eca43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading example embeddings for Liebherr\n",
      "torch.Size([3993, 768])\n",
      "Testing Liebherr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 63/80 [03:22<00:55,  3.28s/it, IoU=0.502]"
     ]
    }
   ],
   "source": [
    "iou_scores = []\n",
    "for name, dataloader in dataloaders.items():\n",
    "    print(\"Loading example embeddings for\", name)\n",
    "    example_embeddings = get_example_embeddings(example_sets[name]).to(device)\n",
    "    print(\"Testing\", name)\n",
    "    iou_score = evaluate_model(model, dataloader, example_embeddings, device)\n",
    "    print('IoU: {:.4f}'.format(iou_score), flush=True)\n",
    "    iou_scores.append(iou_score)\n",
    "print(iou_scores, np.mean(iou_scores[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c25041-92f2-4b19-a53a-f18447a938d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupytervenv)",
   "language": "python",
   "name": "jupytervenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
