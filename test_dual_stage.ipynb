{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad274da1-82b6-4974-bd52-22983ea73ad9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.nn.parallel import DataParallel\n",
    "from torchvision.transforms import ToTensor, Compose\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import os\n",
    "from torchmetrics.classification import BinaryJaccardIndex\n",
    "\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "\n",
    "from datasets import Embedding_Dataset, Custom_Dataset, Cutout_Dataset\n",
    "from utils import SAMPreprocess, PILToNumpy, NumpyToTensor, SamplePoint, embedding_collate, is_valid_file\n",
    "from utils import create_cutouts\n",
    "from models import SAM_Baseline\n",
    "\n",
    "jaccard = BinaryJaccardIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b0e2256-94f7-4f98-99bf-c4d6ceeb318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "folder_paths = [\n",
    "    #'/pfs/work7/workspace/scratch/ul_xto11-FSSAM/Liebherr/dataset',\n",
    "    '/pfs/work7/workspace/scratch/ul_xto11-FSSAM/FSSAM Datasets/EgoHOS/dataset',\n",
    "    '/pfs/work7/workspace/scratch/ul_xto11-FSSAM/FSSAM Datasets/GTEA/dataset',\n",
    "    '/pfs/work7/workspace/scratch/ul_xto11-FSSAM/FSSAM Datasets/LVIS/dataset',\n",
    "    '/pfs/work7/workspace/scratch/ul_xto11-FSSAM/FSSAM Datasets/NDIS Park/dataset',\n",
    "    '/pfs/work7/workspace/scratch/ul_xto11-FSSAM/FSSAM Datasets/TrashCan/dataset',\n",
    "    '/pfs/work7/workspace/scratch/ul_xto11-FSSAM/FSSAM Datasets/ZeroWaste-f/dataset',\n",
    "]\n",
    "select_mode = 'max_sim' # must be one of: 'random', 'first', 'highest_pred', 'max_sim'\n",
    "embed_model_type = 'dino'\n",
    "visual_prompt_engineering = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed91e000-6e9e-477b-b577-192a81646822",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SAM_Baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb7ac1e3-71e6-488d-9129-410103c2c064",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ul/ul_student/ul_xto11/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/home/ul/ul_student/ul_xto11/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/home/ul/ul_student/ul_xto11/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/home/ul/ul_student/ul_xto11/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DINO loaded 336\n"
     ]
    }
   ],
   "source": [
    "if embed_model_type == 'clip':\n",
    "    import clip\n",
    "    embed_model, _ = clip.load(\"ViT-L/14@336px\", device=device) # Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "    cutout_size = embed_model.visual.input_resolution\n",
    "    embed_model.to(device)\n",
    "    print(\"CLIP loaded\", cutout_size)\n",
    "elif embed_model_type == 'dino':\n",
    "    class DINO:\n",
    "        def __init__(self, device):\n",
    "            self.model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14')\n",
    "            self.model.to(device)\n",
    "        def encode_image(self, image):\n",
    "            with torch.no_grad():\n",
    "                return self.model(image)\n",
    "    embed_model = DINO(device)\n",
    "    cutout_size = 336\n",
    "    print(\"DINO loaded\", cutout_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb797d8d-1b19-47ae-abcd-595115039846",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIXEL_MEAN = (0.48145466, 0.4578275, 0.40821073)\n",
    "PIXEL_STD = (0.26862954, 0.26130258, 0.27577711)\n",
    "pixel_mean = torch.Tensor(PIXEL_MEAN).reshape(3, 1, 1)\n",
    "pixel_std = torch.Tensor(PIXEL_STD).reshape(3, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "250fd898-9523-4679-8dee-6e9c34649563",
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_transform = ResizeLongestSide(model.img_size)\n",
    "target_transform = Compose([\n",
    "    sam_transform.apply_image_torch, # rescale\n",
    "    SAMPreprocess(model.img_size, normalize=False), # padding\n",
    "    SamplePoint(),\n",
    "])\n",
    "transform = Compose([\n",
    "    PILToNumpy(),\n",
    "    sam_transform.apply_image, # rescale\n",
    "    NumpyToTensor(),\n",
    "    SAMPreprocess(model.img_size) # padding\n",
    "])\n",
    "example_transform = Compose([\n",
    "    ToTensor(),\n",
    "])\n",
    "\n",
    "def create_dataloader(folder_path):\n",
    "    dataset = Embedding_Dataset(root=folder_path, transform=transform, target_transform=target_transform, is_valid_file=is_valid_file)\n",
    "    example_set = Custom_Dataset(root=folder_path, transform=example_transform, is_valid_file=is_valid_file)\n",
    "\n",
    "    dataset_size = len(dataset)\n",
    "    example_size = int(0.25 * dataset_size)\n",
    "    test_size = dataset_size - example_size\n",
    "    generator = torch.Generator().manual_seed(42)\n",
    "    test_indices, example_indices = random_split(range(len(dataset)), [test_size, example_size], generator=generator)\n",
    "\n",
    "    test_set = Subset(dataset, test_indices)\n",
    "    example_set = Subset(example_set, example_indices)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, collate_fn=embedding_collate)\n",
    "    return test_loader, example_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1151c254-5b65-4551-acb8-12baac02e67c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EgoHOS loaded\n",
      "GTEA loaded\n",
      "LVIS loaded\n",
      "NDIS Park loaded\n",
      "TrashCan loaded\n",
      "ZeroWaste-f loaded\n"
     ]
    }
   ],
   "source": [
    "dataloaders = {}\n",
    "example_sets = {}\n",
    "\n",
    "for folder_path in folder_paths:\n",
    "    dataset_name = folder_path.split('/')[-2]  # Extract dataset name from the folder path\n",
    "    dataloaders[dataset_name], example_sets[dataset_name] = create_dataloader(folder_path)\n",
    "    print(dataset_name, \"loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b416e2d5-ab38-4e5e-9cd0-00929db90a97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import GaussianBlur\n",
    "\n",
    "class CreateCutouts(object):\n",
    "    def __init__(self, cutout_size, padding, background_transform, background_intensity):\n",
    "        self.cutout_size = cutout_size\n",
    "        self.padding = padding\n",
    "        self.background_transform = background_transform\n",
    "        self.background_intensity = background_intensity\n",
    "\n",
    "    def __call__(self, image, masks):\n",
    "        #image = (image - pixel_mean) / pixel_std # better performance without normilization\n",
    "        return create_cutouts(image, masks, self.cutout_size, self.padding, self.background_transform, self.background_intensity)\n",
    "        \n",
    "if visual_prompt_engineering:\n",
    "    background_intensity = .1\n",
    "    background_transform = Compose([\n",
    "        GaussianBlur(11, 10)\n",
    "    ])\n",
    "else:\n",
    "    background_intensity = 0\n",
    "    background_transform = None\n",
    "\n",
    "create_cutouts_f = CreateCutouts(cutout_size, 30, background_transform, background_intensity)\n",
    "\n",
    "#image, masks = example_set[1]\n",
    "#cutouts = create_cutouts_f(image, masks)\n",
    "#plt.imshow(cutouts[0].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86489a7b-a2da-44c5-ac69-79ff00718c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cutouts_dataset(dataset):\n",
    "    cutouts_set = []\n",
    "    for image, masks in tqdm.tqdm(dataset):\n",
    "        cutouts = create_cutouts_f(image, masks)\n",
    "        cutouts_set.append(cutouts)\n",
    "    cutouts_set = Cutout_Dataset(torch.cat(cutouts_set))\n",
    "    return cutouts_set\n",
    "\n",
    "def get_cutout_embeddings(dataset, model, device, batch_size):\n",
    "    cutouts_loader = DataLoader(dataset, batch_size=batch_size)\n",
    "    examples = []\n",
    "    for cutouts in tqdm.tqdm(cutouts_loader):\n",
    "        cutouts = cutouts.to(device)\n",
    "        cutout_embeddings = model.encode_image(cutouts)\n",
    "        cutout_embeddings = cutout_embeddings[~torch.any(cutout_embeddings.isnan(), dim=1)] # remove nan embeddings\n",
    "        examples.append(cutout_embeddings.detach().cpu())\n",
    "    return torch.cat(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d35ac2b-220e-4b26-8d96-e514109fcd88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_example_embeddings(example_set):\n",
    "    if visual_prompt_engineering:\n",
    "        file = 'example_embeddings/' + name + '_vpe_' + embed_model_type + '.pt'\n",
    "    else:\n",
    "        file = 'example_embeddings/' + name + '_' + embed_model_type + '.pt'\n",
    "\n",
    "    if os.path.exists(file):\n",
    "        example_embeddings = torch.load(file)\n",
    "    else:\n",
    "        cutouts_set = get_cutouts_dataset(example_set)\n",
    "        example_embeddings = get_cutout_embeddings(cutouts_set, embed_model, device, batch_size=16)\n",
    "        torch.save(example_embeddings, file)\n",
    "        \n",
    "    print(example_embeddings.shape)\n",
    "    return example_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "516a271d-fe82-4eeb-a640-5f68e911b4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest(images, masks, examples, create_cutouts_f, model, device):\n",
    "    embeddings = []\n",
    "    for image, mask_suggestions in zip(images, masks):\n",
    "        cutouts = create_cutouts_f(image, mask_suggestions).to(device)\n",
    "        cutout_embeddings = model.encode_image(cutouts)\n",
    "        embeddings.append(cutout_embeddings.detach())\n",
    "    embeddings = torch.cat(embeddings)\n",
    "    examples = examples.to(device)\n",
    "    \n",
    "    embeddings /= torch.norm(embeddings, dim=1, keepdim=True)\n",
    "    examples /= torch.norm(examples, dim=1, keepdim=True)\n",
    "    similarity_matrix = embeddings @ examples.T\n",
    "    \n",
    "    similarity_matrix = similarity_matrix.reshape(len(images), 3, len(examples))\n",
    "    similarity_matrix, _ = torch.max(similarity_matrix, dim=-1)\n",
    "    max_i = torch.argmax(similarity_matrix, dim=-1)\n",
    "    return max_i.cpu().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2715068-f052-4b8a-ae21-c91f86f9bb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAM loaded\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = DataParallel(model)\n",
    "model.to(device)\n",
    "print(\"SAM loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d58a8a23-2806-450d-8633-0ac87c3bbbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, example_embeddings, device): # select_mode, create_cutouts_f, embed_model\n",
    "    np.random.seed(42)\n",
    "    model.eval()\n",
    "    total_iou = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm.tqdm(dataloader)\n",
    "        for images, masks, points, embeddings in pbar:\n",
    "            embeddings = embeddings.to(device)\n",
    "            points = points.to(device)\n",
    "\n",
    "            pred_masks, pred_iou = model(embeddings, points)\n",
    "            pred_masks, pred_iou = pred_masks.cpu(), pred_iou.cpu()\n",
    "            pred_masks = pred_masks > 0\n",
    "            \n",
    "            if select_mode == 'random':\n",
    "                max_i = torch.randint(3, (len(images),))\n",
    "            elif select_mode == 'first':\n",
    "                max_i = torch.zeros((len(images),), dtype=int)\n",
    "            elif select_mode == 'highest_pred':\n",
    "                max_i = torch.argmax(pred_iou, dim=1)\n",
    "            elif select_mode == 'max_sim':\n",
    "                max_i = get_closest(images, pred_masks, example_embeddings, create_cutouts_f, embed_model, device)\n",
    "\n",
    "            pred_masks = pred_masks[range(len(pred_masks)), max_i].unsqueeze(1)\n",
    "            iou = jaccard(masks, pred_masks)\n",
    "\n",
    "            total_iou += iou * images.size(0)\n",
    "            total_samples += images.size(0)\n",
    "            \n",
    "            pbar.set_postfix({'IoU': (total_iou / total_samples).item()})\n",
    "\n",
    "    average_iou = total_iou / total_samples\n",
    "    return average_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1d8597-68ff-4630-9f41-67c0ab4eca43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name, dataloader in dataloaders.items():\n",
    "    print(\"Loading example embeddings for\", name)\n",
    "    example_embeddings = get_example_embeddings(example_sets[name])\n",
    "    print(\"Testing\", name)\n",
    "    iou_score = evaluate_model(model, dataloader, example_embeddings, device)\n",
    "    print('IoU: {:.4f}'.format(iou_score), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ab4fc3-b15b-4ef8-8db0-ece8f2f1bf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM = [0.4695, 0.5637, 0.3954, 0.4686, 0.1120, 0.2694]\n",
    "print(np.mean(RANDOM))\n",
    "\n",
    "FIRST = [0.4372, 0.4609, 0.4185, 0.5209, 0.0588, 0.3641]\n",
    "print(np.mean(FIRST))\n",
    "\n",
    "HIGHEST = [0.5349, 0.7512, 0.4374, 0.4321, 0.1903, 0.2587]\n",
    "print(np.mean(HIGHEST))\n",
    "\n",
    "##### CLIP #####\n",
    "MAX_SIM = [0.4979, 0.6253, 0.4101, 0.3702, 0.2812, 0.2767]\n",
    "print(np.mean(MAX_SIM))\n",
    "\n",
    "VPE = [0.5338, 0.8117, 0.4168, 0.3844, 0.2712, 0.2645]\n",
    "print(np.mean(VPE))\n",
    "\n",
    "##### DINO #####\n",
    "MAX_SIM = [0.5388, 0.7259, 0.4657, 0.6028, 0.3367, 0.2933]\n",
    "print(np.mean(MAX_SIM))\n",
    "\n",
    "VPE = [0.5079, 0.7455, 0.4574, 0.5149, 0.3457, 0.3003]\n",
    "print(np.mean(VPE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4016b40f-b09f-40ed-af79-5d974a8347c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupytervenv)",
   "language": "python",
   "name": "jupytervenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
